---
title: "Predicting Pokémon Types Using Clustering and Classification"
subtitle: "Code Submission"
author: "Justin Zhang, Isaac Baguisa, Alex Faassen"
date: "2025-04-04"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Note:** Commented out unnecessary "test" outputs.

# Data

## Pre-processing

```{r}
library(png)
library(ggplot2)
library(dplyr)

setwd("Data")

## Load Stats
stats = read.csv("pokemon_stats.csv", header = TRUE)
# Check
# str(stats)
# Image path
stats$image_path = paste0("images/", tolower(stats$name.simple), ".png")
# Check
# head(stats$image_path)

## Load Image Registry
img_reg = read.csv("pokemon_img.csv", header = TRUE)
# Has Image
stats$has_img = tolower(stats$name.simple) %in% img_reg$Name
# No Img
stats[!stats$has_img, "name"]

## Load Images
# Test: Abomasnow
abomasnow = readPNG("images/abomasnow.png")
ggplot()+
  annotation_raster(abomasnow,xmin=-Inf,xmax=+Inf,ymin=-Inf,ymax= +Inf)
# Pixels
# str(abomasnow)
pix = prod(dim(abomasnow)[1:2])

# Image List
image_list = list.files("images", pattern = "*.png") %>% paste("images/", ., sep = "")

# Helper fn
flatten_img = function(img_path) {
  img = readPNG(img_path) # Read image as raster array (dim: [120, 120, 4])
  return(as.vector(img[,,-4])) # Flatten to vector + Remove Alpha
}

# Image Dataset
images = sapply(image_list, flatten_img) %>% t() %>% as.data.frame() %>%
  mutate(image_path = image_list) %>% select(image_path, everything())
# colnames(images) = c("image_path", rep())
# Check
images[1:5, 1:5]
# str(images)

## Match Datasets
# Match the rows to represent the same pokemon, in the same order.
images = images %>%
  semi_join(stats, by = "image_path") %>%
  arrange(match(image_path, stats$image_path))
# Check
mean(stats$image_path == images$image_path)

## Save Datasets
# save(stats, images, file = "pokemon.RData")

setwd("..")
```

## EDA Visuals

```{r}
# Libraries
library(png)
library(ggplot2)
library(patchwork)

## Example Pokemon
ex1 = readPNG("Data/images/pikachu.png")
ex2 = readPNG("Data/images/charizard.png")
p1 = ggplot()+
  annotation_raster(ex1,xmin=-Inf,xmax=+Inf,ymin=-Inf,ymax= +Inf) +
  labs(title = "Pikachu (Primary Type: Electric)")
p2 = ggplot()+
  annotation_raster(ex2,xmin=-Inf,xmax=+Inf,ymin=-Inf,ymax= +Inf) +
  labs(title = "Charizard (Primary Type: Fire)")
p1 + p2



## Top Pokemon Types: Bar Chart + Attack Boxplot
type_cols <- c(
  "bug" = "#A8B820", "dark" = "#705848", "dragon" = "#7038F8", "electric" = "#F8D030",
  "fairy" = "#EE99AC", "fighting" = "#C03028", "fire" = "#F08030", "flying" = "#A890F0",
  "ghost" = "#705898", "grass" = "#78C850", "ground" = "#E0C068", "ice" = "#98D8D8",
  "normal" = "#A8A878", "poison" = "#A040A0", "psychic" = "#F85888", "rock" = "#B8A038",
  "steel" = "#B8B8D0", "water" = "#6890F0"
)

# Top N primary types (by freq)
top5_types <- stats %>%
  mutate(type1 = tolower(type1)) %>%
  count(type1, sort = TRUE) %>%
  top_n(5, n)

filtered_data <- stats %>%
  mutate(type1 = tolower(type1)) %>%
  filter(type1 %in% top5_types$type1) %>%
  mutate(type1 = fct_infreq(type1)) 

# Bar chart of pokemon by primary type
type_freq = ggplot(filtered_data, aes(x = fct_infreq(type1), fill = type1)) +
  geom_bar() +
  scale_fill_manual(values = type_cols) +
  labs(
    title = "Top 5 Pokémon Types",
    x = "Primary Type",
    y = "Count"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "left")

# Boxplot of Attack by Type
type_attack = ggplot(filtered_data, aes(x = type1, y = attack, fill = type1)) +
  geom_boxplot() +
  scale_fill_manual(values = type_cols) +
  labs(
    title = "Attack Stat Distributions",
    x = "Primary Type",
    y = "Attack Stat"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")

## Table of summary stats for key features
key_features <- c("hp", "attack", "defense", "sp_attack", "sp_defense", "speed", "height_m", "weight_kg")

# Summary stats
summary_table <- stats %>%
  select(all_of(key_features)) %>%
  summarise(across(everything(), list(
    mean = ~round(mean(.x, na.rm = TRUE), 2),
    sd = ~round(sd(.x, na.rm = TRUE), 2),
    min = ~min(.x, na.rm = TRUE),
    q1 = ~quantile(.x, 0.25, na.rm = TRUE),
    median = ~median(.x, na.rm = TRUE),
    q3 = ~quantile(.x, 0.75, na.rm = TRUE),
    max = ~max(.x, na.rm = TRUE)
  ), .names = "{.col}_{.fn}")) %>%
  pivot_longer(cols = everything(),
               names_to = c("Feature", "Statistic"),
               names_pattern = "^(.*)_(mean|sd|min|q1|median|q3|max)$",
               values_to = "Value") %>%
  pivot_wider(names_from = Statistic, values_from = Value)

# Save
save(p1, p2, type_freq, type_attack, summary_table, file = "Figures/data_description.RData")
```

# Image Dimension Reduction

```{r}
library(patchwork)
library(ggfortify)

## Helper functions
barplot = function(values){
  n = length(values)
  df = data.frame(value = values,  index = 1:n)
  ggplot(df, aes(index, value, fill = value)) + 
    geom_bar(color = "black", stat = "identity")+
    scale_fill_gradient2(low="#619CFF", mid="white", high="#F8766D")+
    theme_bw()
}

getImg = function(flat_img){
  # matrix(unlist(gsvalues), 120, 120, 4, byrow = T)
  # rasterGrob(array(flat_img, dim = c(120, 120, 4)))
  array(as.numeric(flat_img), dim = c(120, 120, 3)) # (120, 120, 4)
}

plotImg <- function(img_raster) {
  # Plot using ggplot2
  ggplot() +
    annotation_raster(img_raster, xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf) +
    theme_void()  # Remove axes for clean visualization
}
```

## PCA

```{r}
## PCA
# Defaults: center = TRUE, scale. = FALSE
PCA = prcomp(images[,-1], center = TRUE, scale. = FALSE) #scale = TRUE)

sum_PCA = summary(PCA)
```

```{r}
## Biplot
# Colours
stats$type1 = factor(stats$type1)
# unique(stats$type1)
type_colours = c("#7AC74C", "#EE8130", "#6390F0", "#A6B91A", "#A8A77A", "#A33EA1",
            "#F7D02C", "#E2BF65", "#D685AD", "#CC2E28", "#F95587", "#B6A136", "#735797",
            "#96D9D6", "#6F35FC", "#705746", "#B7B7CE", "#A98FF3")
names(type_colours) = unique(stats$type1)

# library(ggfortify)
autoplot(PCA, data = cbind(stats$name, stats$type1, images[,-1]), shape = FALSE, color = "stats$type1", label = TRUE, label.label = "stats$name", scale = 0) +
  scale_colour_manual(values = type_colours) +
  theme(legend.position = "none")

# Without labels, just types.
autoplot(PCA, data = cbind(stats$name, stats$type1, images[,-1]), label = FALSE, shape = 16, color = "stats$type1", scale = 0) +
  scale_colour_manual(values = type_colours) +
  theme(legend.position = "none")
```

## Loading Vectors

```{r}
## Smallest PC1 Pokemon
# Extract PCA-transformed data
pca_data <- as.data.frame(PCA$x)  # Convert PCA results to a data frame

# Add Pokémon names and types for reference
pca_data$name <- stats$name  # Ensure stats$name contains Pokémon names
pca_data$type1 <- stats$type1  # Primary type for reference

# Sort by PC1 in ascending order and select the two smallest
smallest_pc1_pokemon <- pca_data[order(pca_data$PC1), ][1:2, ]

# Print result
print(rownames(smallest_pc1_pokemon))
```

```{r}
## PC1
# Extremes
pos1 = plotImg(getImg(images[stats$name == "Wailord", -1]))
pos2 = plotImg(getImg(images[stats$name == "Abomasnow", -1]))
neg1 = plotImg(getImg(images[stats$name == "Elgyem", -1])) # Pikipek
neg2 = plotImg(getImg(images[stats$name == "Geodude", -1])) # Unown
# exPlots = sapply(c(pos1, pos2, neg1, neg2), function(rgba) plotImg(getImg(rgba)))
# (pos1 + pos2) / (neg1 + neg2)
final_plot = (pos1 + labs(title = "Most Positive PC1: Wailord") +
              pos2 + labs(title = "Second Positive PC1: Abomasnow")) /
             (neg1 + labs(title = "Most Negative PC1: Elgyem") +
              neg2 + labs(title = "Second Negative PC1: Geodude"))
print(final_plot)

# Loading Vector
pca = PCA$rotation[,1]
pca_norm = (pca - min(pca)) / (max(pca) - min(pca))
# pca_norm = (pca - mean(pca))/(2 * sd(pca))
# pca_clip = pmax(pmin(pca, 1), 0)
plotImg(getImg(pca_norm))
```

```{r}
## PC2
# Extremes
pos1 = plotImg(getImg(images[stats$name == "Slowking", -1]))
pos2 = plotImg(getImg(images[stats$name == "Gardevoir", -1]))
neg1 = plotImg(getImg(images[stats$name == "Altaria", -1]))
neg2 = plotImg(getImg(images[stats$name == "Swablu", -1]))
# exPlots = sapply(c(pos1, pos2, neg1, neg2), function(rgba) plotImg(getImg(rgba)))
# (pos1 + pos2) / (neg1 + neg2)
final_plot = (pos1 + labs(title = "Most Positive PC1: Slowking") +
              pos2 + labs(title = "Second Positive PC1: Gardevoir")) /
             (neg1 + labs(title = "Most Negative PC1: Altaria") +
              neg2 + labs(title = "Second Negative PC1: Swablu"))
print(final_plot)

# Loading Vector
pca = PCA$rotation[,2]
pca_norm = (pca - min(pca)) / (max(pca) - min(pca))
plotImg(getImg(pca_norm))
```

## Variance Explained

```{r}
## Variance Explained
ve = summary(PCA)$importance[3,]

## Retain PCs
var_retain = 0.8 # % of VE
num_pcs = min(which(ve >= var_retain)); num_pcs
dr_images = data.frame(
  image_path = images$image_path,
  PCA$x[,1:num_pcs]
)

# Plot
barplot(ve)+
  xlab("principal component")+
  ylab("cumulative variance explained")+
  ylim(0, 1)+
  geom_hline(aes(yintercept = 0.9), linewidth = 1, linetype = "dashed") +
  geom_hline(aes(yintercept = 0.95), linewidth = 1, linetype = "dashed", color = "red") +
  geom_vline(xintercept = num_pcs, linetype = "dotted", color = "blue") +  # Highlight selected PC count
  annotate("text", x = num_pcs + 5, y = 0.5, label = paste0(num_pcs, " PCs retained"), color = "blue")
```

## Visualize Compressed Images

```{r}
## Abomasnow
pokemon = which(stats$name == "Abomasnow")
# Original image
og = plotImg(getImg(images[pokemon, -1])) + labs(title = "Original")
# Compressed image
scores = (as.numeric(images[pokemon, -1]) - PCA$center) %*% PCA$rotation
scores_95VE = scores[, 1:num_pcs, drop = FALSE] %*% t(PCA$rotation[, 1:num_pcs]) + PCA$center
# scores_95VE_norm = (scores_95VE - min(scores_95VE)) / (max(scores_95VE) - min(scores_95VE))
scores_95VE_clip = pmax(pmin(scores_95VE, 1), 0)
compressed = plotImg(getImg(scores_95VE_clip)) +
  labs(title = paste("PCA Reconstructed (", num_pcs, " PCs)", sep=""))
# Side by side
sbs = og + compressed
print(sbs)

## Gardevoir
pokemon = which(stats$name == "Gardevoir")
# Original image
og = plotImg(getImg(images[pokemon, -1])) + labs(title = "Original")
# Compressed image
scores = (as.numeric(images[pokemon, -1]) - PCA$center) %*% PCA$rotation
scores_95VE = scores[, 1:num_pcs, drop = FALSE] %*% t(PCA$rotation[, 1:num_pcs]) + PCA$center
# scores_95VE_norm = (scores_95VE - min(scores_95VE)) / (max(scores_95VE) - min(scores_95VE))
scores_95VE_clip = pmax(pmin(scores_95VE, 1), 0)
compressed = plotImg(getImg(scores_95VE_clip)) +
  labs(title = paste("PCA Reconstructed (", num_pcs, " PCs)", sep=""))
# Side by side
sbs = og + compressed
print(sbs)
```

# Clustering

## Setup

```{r}
set.seed(2201)
# Load libraries and setup
library(cluster)
library(factoextra)
library(caret)
library(ggplot2)
library(ggfortify)
library(tidyverse)
library(VIM)
library(gridExtra)
load("Data/pokemon.RData")
load("Data/dr_pokemon2.RData")
load("DimensionReduction/umap_pokemon.rds")

stats_numeric <- stats %>%
  select(-c(abilities, capture_rate, classfication, japanese_name, name,
            name.simple, type1, type2, image_path, has_img))

#Imputation on missing data using KNN
stats_numeric_impKNN <- kNN(stats_numeric)
#Remove cols with variance = 0
# Check the variance of each column
variances <- apply(stats_numeric_impKNN, 2, var)
zero_var_col <- which(variances == 0)
stats_numeric_impKNN <- stats_numeric_impKNN[, -zero_var_col]
```

## Helper functions
```{r}
# Helper function lecture 7
scatterplot = function(X, M, cluster, label = FALSE){
  X_df <- data.frame(X, cluster = as.factor(cluster))  
  M_df <- data.frame(M) 
  
  if (length(unique(cluster)) == 1) {
    plt <- ggplot(X_df, aes(x = PC1, y = PC2)) +
      geom_point() +
      geom_point(data = M_df, aes(x = PC1, y = PC2), shape = 4, size = 4, color = "red") +
      labs(title = "Scatterplot of Pokemon Clusters")
    
    if (label) {
      plt <- plt + geom_text(aes(label = stats$name), nudge_x = 0.1, size = 3)
    }
    return(plt)
  } 
  else {
    ggplot(X_df, aes(x = PC1, y = PC2, color = cluster)) +
      geom_point(alpha = 0.7) +
      geom_point(data = M_df, aes(x = PC1, y = PC2), shape = 4, size = 4, color = "black") +
      scale_color_manual(values = rainbow(length(unique(cluster)))) +
      theme_minimal() +
      labs(title = "K-Means Clustering of Pokemon (PC1 vs PC2)", x = "PC1", y = "PC2") +
      theme(legend.position = "right")
  }
}
#Reference lecture 7 slide 10, and modifications from hw 3
weighted_kmeans <- function(X, K) {
  X <- as.matrix(X)  
  n <- nrow(X)  
  
  # Step 1: Initialize first centroid  
  set.seed(2201)
  centroids <- X[sample(1:n, 1), , drop = FALSE]  
  
  # Step 2: Compute the initial weights distance 
  distances <- as.matrix(dist(rbind(centroids, X)))[2:(n+1), 1]  
  weights <- distances^2 / sum(distances^2) 
  
  # Step 3: Initialize the remaining K-1 centroids
  for (i in 2:K) {
    P <- weights / sum(weights)  
    pick <- sample(1:n, 1, prob = P)  
    new_centroid <- X[pick, , drop = FALSE]
    centroids <- rbind(centroids, new_centroid)
    
    # Compute new weights based on the distance from the new centroid
    new_distances <- as.matrix(dist(rbind(new_centroid, X)))[2:(n+1), 1]
    weights <- new_distances^2 / sum(new_distances^2)  
  }
  
  converged <- FALSE
  clusters <- rep(0, n)
  while (!converged) {
    # Assign points to the nearest centroid based on weighted distances
    distances <- as.matrix(dist(rbind(centroids, X)))  
    distances <- distances[1:K, (K+1):(n+K)]  
    
    # Weight the distances using the computed weights (soft clustering)
    weighted_distances <- distances * weights
    
    # Assign each point to the nearest centroid based on weighted distances
    new_clusters <- apply(weighted_distances, 2, which.min)
    
    # Update centroids using weighted averages
    new_centroids <- matrix(NA, ncol = ncol(X), nrow = K)
    for (k in 1:K) {
      cluster_points <- X[new_clusters == k, ]  
      cluster_weights <- weights[new_clusters == k]  
      if (length(cluster_points) > 0) {
        new_centroids[k, ] <- colSums(cluster_points * cluster_weights) / sum(cluster_weights)
      }
    }
    
    # Check for convergence (if centroids don't change)
    if (all(centroids == new_centroids)) {
      converged <- TRUE
    } else {
      centroids <- new_centroids  
      clusters <- new_clusters 
    }
  }

  
  return(list(centroids = centroids, clusters = clusters))
}

Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
```

## Clustering on Stats Dataset

```{r}
# PCA on stats
pca_stats <- prcomp(stats_numeric_impKNN, center = TRUE, scale. = TRUE)
pca_stats_df <- as.data.frame(pca_stats$x)
colnames(pca_stats_df) <- paste0("PC", 1:ncol(pca_stats_df))
var_explained <- summary(pca_stats)$importance[2, ]  # Proportion of variance explained
cumulative_var <- cumsum(var_explained)
# Keep 20 PCs (90% VE)
pca_stats_df <- pca_stats_df[,1:20]
```

```{r}
## Kmeans on stats k = 18
k_types <- length(unique(stats$type1)) 
kmeans_stats <- kmeans(pca_stats_df, centers = k_types)
scatterplot(pca_stats_df, kmeans_stats$centers, kmeans_stats$cluster)
```

```{r}
## Comparing standard kmeans with kmeans++ and weighted kmeans

#Kmeans++

#Ref: lecture 7
# Randomly select the first centroid
n <- nrow(pca_stats_df) 
M <- pca_stats_df[sample(1:n, 1), , drop = FALSE]

for (i in 2:k_types) {
  # Dist from each point to the nearest centroid
  D <- as.matrix(dist(rbind(M, pca_stats_df)))[2:(n+1), 1] 
  
  # Probability for each point to be chosen as the next centroid
  P <- D^2 / sum(D^2)

  # Select the next centroid based on P
  pick <- sample(1:n, 1, prob = P)
  M <- rbind(M, pca_stats_df[pick, , drop = FALSE])
}

kmeans_stats_pp <- kmeans(pca_stats_df, centers = M, algorithm = "Lloyd")
scatterplot(pca_stats_df, kmeans_stats_pp$centers, kmeans_stats_pp$cluster) 

```

```{r}
#Lower withinss is better
#kmeans_stats$tot.withinss
#kmeans_stats_pp$tot.withinss

#Higher betweenss is better
#kmeans_stats$betweenss
#kmeans_stats_pp$betweenss

#Standard kmeans outperforms kmeans++
```

```{r}
# Weighted kmeans
K <- 18
kmeans_w_stats <- weighted_kmeans(pca_stats_df, K)
colnames(kmeans_w_stats$centroids) <- colnames(pca_stats_df)

ggplot(data.frame(pca_stats_df), aes(PC1, PC2, color = factor(kmeans_w_stats$clusters))) + 
  geom_point(size = 3) +
  geom_point(data = data.frame(kmeans_w_stats$centroids), aes(PC1, PC2), 
             color = "black", shape = 4, size = 5) +
  labs(title = "Weighted K-means Clustering", x = "PC1", y = "PC2")
```

```{r}
## Tune for optimal K

# Elbow method
fviz_nbclust(pca_stats_df, kmeans, method = "wss", algorithm = "Lloyd")

# Silhouette method
fviz_nbclust(pca_stats_df, kmeans, method = "silhouette") 

#CH index
CHs = c()
Ks = seq(1, 20, 2)  

for(K in Ks){
  KM = kmeans(pca_stats_df, centers = k_types, nstart = 25)
   
  # Between-cluster sum of squares
  B = KM$betweenss

  # Within-cluster sum of squares
  W = KM$tot.withinss
  
  # Number of data points
  n = nrow(pca_stats_df)
  
  # Calculate the Calinski-Harabasz index
  CH = (B / (K - 1)) / (W / (n - K))
  
  # Append the CH index for the current K to the list
  CHs = c(CHs, CH)
}
df = data.frame(K = Ks, CH = CHs)
ggplot(df, aes(K, CH)) +
  geom_point() +
  geom_line() +
  ylab("CH index")

# Gap statistics
gapstat_stats = clusGap(pca_stats_df, FUN = kmeans, nstart = 50, K.max = 10, B = 50)
fviz_gap_stat(gapstat_stats, maxSE = list(method = "Tibs2001SEmax", SE.factor = 1)) 

# Elbow method: Optimal K = 3
# 
# Silhouette method: Optimal K = 2
# 
# CH Index: Optimal K = 1
# 
# Gap stats: Optimal K = 10
# 
```

```{r}
# Kmeans on optimal K = 3

kmeans_stats_optimal <- kmeans(pca_stats_df, centers = 3)
scatterplot(pca_stats_df, kmeans_stats_optimal$centers, kmeans_stats_optimal$cluster)
```

```{r}
## Comparing with true labels

comp_labels <- data.frame(type1 = stats$type1)
comp_labels$cluster <- as.factor(kmeans_stats$cluster)

# Map clusters to true labels
cluster_label_mapping <- comp_labels %>%
  mutate(cluster = factor(kmeans_stats$cluster)) %>%
  group_by(cluster) %>%
  summarise(mode_type = Mode(type1), .groups = 'drop') 


# Map cluster labels to the most common actual type
labelled_clusters <- comp_labels %>%
  left_join(cluster_label_mapping, by = "cluster") %>%
  select(type1, cluster, mode_type)

# Calculate accuracy 
#mean(labelled_clusters$type1 == labelled_clusters$mode_type) 
```

```{r}
comp_labels <- data.frame(type1 = stats$type1)
comp_labels$cluster <- as.factor(kmeans_stats_pp$cluster)

# Map clusters to true labels
cluster_label_mapping <- comp_labels %>%
  mutate(cluster = factor(kmeans_stats_pp$cluster)) %>%
  group_by(cluster) %>%
  summarise(mode_type = Mode(type1), .groups = 'drop') 


# Map cluster labels to the most common actual type
labelled_clusters <- comp_labels %>%
  left_join(cluster_label_mapping, by = "cluster") %>%
  select(type1, cluster, mode_type)

# Calculate accuracy 
#mean(labelled_clusters$type1 == labelled_clusters$mode_type) 
```

```{r}
comp_labels <- data.frame(type1 = stats$type1)
comp_labels$cluster <- as.factor(kmeans_w_stats$cluster)

# Map clusters to true labels
cluster_label_mapping <- comp_labels %>%
  mutate(cluster = factor(kmeans_w_stats$cluster)) %>%
  group_by(cluster) %>%
  summarise(mode_type = Mode(type1), .groups = 'drop') 


# Map cluster labels to the most common actual type
labelled_clusters <- comp_labels %>%
  left_join(cluster_label_mapping, by = "cluster") %>%
  select(type1, cluster, mode_type)

# Calculate accuracy  (13 types)
#mean(labelled_clusters$type1 == labelled_clusters$mode_type) 
```

```{r}
comp_labels$cluster <- as.factor(kmeans_stats_optimal$cluster)

# Map clusters to true labels
cluster_label_mapping <- comp_labels %>%
  mutate(cluster = factor(kmeans_stats_optimal$cluster)) %>%
  group_by(cluster) %>%
  summarise(mode_type = Mode(type1), .groups = 'drop') 


# Map cluster labels to the most common actual type
labelled_clusters <- comp_labels %>%
  left_join(cluster_label_mapping, by = "cluster") %>%
  select(type1, cluster, mode_type)

# Calculate accuracy 
# mean(labelled_clusters$type1 == labelled_clusters$mode_type)

```

## Clustering on Image Dataset

```{r}
## Kmeans on images k = 18

pca_imgs_df<- dr_images[,-1]  # Remove image_path column
k_types <- length(unique(stats$type1))
kmeans_imgs <- kmeans(pca_imgs_df, centers = k_types)
```

```{r}
## Comparing standard kmeans with kmeans++ and weighted kmeans

#Kmeans++
#Ref: lecture 7
# Randomly select the first centroid
n <- nrow(pca_imgs_df) 
M <- pca_imgs_df[sample(1:n, 1), , drop = FALSE]

# Select remaining k-1 centroids using weighted probability
for (i in 2:k_types) {
  # Compute distance from each point to the nearest centroid
  D <- as.matrix(dist(rbind(M, pca_imgs_df)))[2:(n+1), 1] 
  
  # Probability for each point to be chosen as the next centroid
  P <- D^2 / sum(D^2)

  # Select the next centroid based on P
  pick <- sample(1:n, 1, prob = P)
  M <- rbind(M, pca_imgs_df[pick, , drop = FALSE])
}

kmeans_imgs_pp <- kmeans(pca_imgs_df, centers = M, algorithm = "Lloyd")
```

```{r}
#Lower withinss is better
#kmeans_imgs$tot.withinss
#kmeans_imgs_pp$tot.withinss

#Higher betweenss is better
#kmeans_imgs$betweenss
#kmeans_imgs_pp$betweenss

#Standard kmeans outperforms kmeans++
```

```{r}
#Weighted K-means

K <- 3
kmeans_w_imgs <- weighted_kmeans(pca_imgs_df, K)
colnames(kmeans_w_imgs$centroids) <- colnames(pca_imgs_df)

ggplot(data.frame(pca_imgs_df), aes(PC1, PC2, color = factor(kmeans_w_imgs$clusters))) + 
  geom_point(size = 3) +
  geom_point(data = data.frame(kmeans_w_imgs$centroids), aes(PC1, PC2), 
             color = "black", shape = 4, size = 5) +
  labs(title = "Weighted K-means Clustering", x = "PC1", y = "PC2")
```

```{r}
## Tune for optimal k

# Elbow method
fviz_nbclust(pca_imgs_df, kmeans, method = "wss") 

# Silhouette method
fviz_nbclust(pca_imgs_df, kmeans, method = "silhouette") 

# CH Index
CHs = c()
Ks = seq(1, 10, 1)

for(K in Ks){
  KM = kmeans(pca_imgs_df, centers = k_types, nstart = 25)
  
  # Between-cluster sum of squares
  B = KM$betweenss

  # Within-cluster sum of squares
  W = KM$tot.withinss
  
  # Number of data points
  n = nrow(pca_imgs_df)
  
  # Calculate the Calinski-Harabasz index
  CH = (B / (K - 1)) / (W / (n - K))
  
  # Append the CH index for the current K to the list
  CHs = c(CHs, CH)
}

df = data.frame(K = Ks, CH = CHs)
ggplot(df, aes(K, CH)) +
  geom_point() +
  geom_line() +
  ylab("CH index")

#Gap statistics
gapstat_img = clusGap(pca_imgs_df, FUN = kmeans, nstart = 50, K.max = 10, B = 50)
fviz_gap_stat(gapstat_img, maxSE = list(method = "Tibs2001SEmax", SE.factor = 1)) 

# Elbow method: Optimal K = 3
# 
# Silhouette method: Optimal K = 2
# 
# CH Index: Optimal K = 1
# 
# Gap statistics: Optimal K = 10
```

```{r}
#Kmeans on optimal K = 3

kmeans_imgs_optimal <- kmeans(pca_imgs_df, centers = 3, nstart = 25)
scatterplot(pca_imgs_df, kmeans_imgs_optimal$centers, kmeans_imgs_optimal$cluster)
```

```{r}
## Comparing with true labels

comp_labels$cluster <- as.factor(kmeans_imgs$cluster)

# Map clusters to true labels
cluster_label_mapping <- comp_labels %>%
  mutate(cluster = factor(kmeans_imgs$cluster)) %>%
  group_by(cluster) %>%
  summarise(mode_type = Mode(type1), .groups = 'drop') 

# Map cluster labels to the most common actual type
labelled_clusters <- comp_labels %>%
  left_join(cluster_label_mapping, by = "cluster") %>%
  select(type1, cluster, mode_type)

# Calculate accuracy 
#mean(labelled_clusters$type1 == labelled_clusters$mode_type) 
```

```{r}
comp_labels$cluster <- as.factor(kmeans_imgs_pp$cluster)

# Map clusters to true labels
cluster_label_mapping <- comp_labels %>%
  mutate(cluster = factor(kmeans_imgs_pp$cluster)) %>%
  group_by(cluster) %>%
  summarise(mode_type = Mode(type1), .groups = 'drop') 

# Map cluster labels to the most common actual type
labelled_clusters <- comp_labels %>%
  left_join(cluster_label_mapping, by = "cluster") %>%
  select(type1, cluster, mode_type)

# Calculate accuracy 
#mean(labelled_clusters$type1 == labelled_clusters$mode_type) 
```

```{r}
comp_labels <- data.frame(type1 = stats$type1)
comp_labels$cluster <- as.factor(kmeans_w_imgs$cluster)

# Map clusters to true labels
cluster_label_mapping <- comp_labels %>%
  mutate(cluster = factor(kmeans_w_imgs$cluster)) %>%
  group_by(cluster) %>%
  summarise(mode_type = Mode(type1), .groups = 'drop') 

# Map cluster labels to the most common actual type
labelled_clusters <- comp_labels %>%
  left_join(cluster_label_mapping, by = "cluster") %>%
  select(type1, cluster, mode_type)

# Calculate accuracy
#mean(labelled_clusters$type1 == labelled_clusters$mode_type) 
```

```{r}
comp_labels$cluster <- as.factor(kmeans_imgs_optimal$cluster)

# Map clusters to true labels
cluster_label_mapping <- comp_labels %>%
  mutate(cluster = factor(kmeans_imgs_optimal$cluster)) %>%
  group_by(cluster) %>%
  summarise(mode_type = Mode(type1), .groups = 'drop') 


# Map cluster labels to the most common actual type
labelled_clusters <- comp_labels %>%
  left_join(cluster_label_mapping, by = "cluster") %>%
  select(type1, cluster, mode_type)

# Calculate accuracy 
#mean(labelled_clusters$type1 == labelled_clusters$mode_type) 

```

```{r}
# Clustering with UMAP

umap_data <- UMAP_imgs$layout
colnames(umap_data) <- c("PC1","PC2") #Change col name later  
k_types <- length(unique(stats$type1))
kmeans_imgs_umap <- kmeans(umap_data, centers = k_types, nstart = 25)

```

```{r}
comp_labels$cluster <- as.factor(kmeans_imgs_umap$cluster)

# Map clusters to true labels
cluster_label_mapping <- comp_labels %>%
  mutate(cluster = factor(kmeans_imgs_umap$cluster)) %>%
  group_by(cluster) %>%
  summarise(mode_type = Mode(type1), .groups = 'drop') 

# Map cluster labels to the most common actual type
labelled_clusters <- comp_labels %>%
  left_join(cluster_label_mapping, by = "cluster") %>%
  select(type1, cluster, mode_type)

# Calculate accuracy 
#mean(labelled_clusters$type1 == labelled_clusters$mode_type) 

```

```{r}
#Kmeans++ with UMAP data

set.seed(1234)
n <- nrow(umap_data) 
M <- umap_data[sample(1:n, 1), , drop = FALSE]

# Select remaining k-1 centroids using weighted probability
for (i in 2:k_types) {
  # Compute distance from each point to the nearest centroid
  D <- as.matrix(dist(rbind(M, umap_data)))[2:(n+1), 1] 
  
  # Probability for each point to be chosen as the next centroid
  P <- D^2 / sum(D^2)

  # Select the next centroid based on P
  pick <- sample(1:n, 1, prob = P)
  M <- rbind(M, umap_data[pick, , drop = FALSE])
}

kmeans_umap_pp <- kmeans(umap_data, centers = M, algorithm = "Lloyd")
```

```{r}
comp_labels$cluster <- as.factor(kmeans_umap_pp$cluster)

# Map clusters to true labels
cluster_label_mapping <- comp_labels %>%
  mutate(cluster = factor(kmeans_umap_pp$cluster)) %>%
  group_by(cluster) %>%
  summarise(mode_type = Mode(type1), .groups = 'drop') 

# Map cluster labels to the most common actual type
labelled_clusters <- comp_labels %>%
  left_join(cluster_label_mapping, by = "cluster") %>%
  select(type1, cluster, mode_type)

# Calculate accuracy 
# mean(labelled_clusters$type1 == labelled_clusters$mode_type) 
```




# Classification

```{r}
#
```

# Results

```{r}
#
```

